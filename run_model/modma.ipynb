{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhhaibo2023/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "sys.path.append(os.path.join(project_root, 'code'))\n",
    "\n",
    "from model.dataset import MODMA\n",
    "from model.run_train import run\n",
    "import model.config as config\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2025\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "# 加载数据\n",
    "r_data = torch.load('../data/modma.pt')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.ptm)  # 使用BERT中文分词器\n",
    "\n",
    "dataset_name = 'modma'\n",
    "dataset = MODMA(\n",
    "    dataset_name = dataset_name,\n",
    "    response_texts=r_data['responses'],\n",
    "    labels=r_data['labels'],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=64\n",
    ")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhhaibo2023/program/msce/code/run_model/modma\n"
     ]
    }
   ],
   "source": [
    "LR = 5e-5\n",
    "Weight_Decay = 1e-4\n",
    "EPOCHS = 50\n",
    "BS = 16\n",
    "Dropout = 0.5\n",
    "\n",
    "\n",
    "gammas = [1.0, 0.7072, 0.2005]\n",
    "\n",
    "classify_flag ='bin' # penta\n",
    "\n",
    "logdir_root = os.path.join(os.getcwd(), dataset_name)\n",
    "print(logdir_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_model_path: /home/zhhaibo2023/Pretrained_Model/bert-base-chinese\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DepressionDetectionModel\n",
      "num_stimuls:  18\n",
      "fold: 1, epoch: 1, train_loss: 3.2855, train_acc: 43.9, train_fscore: 34.29, test_loss: 3.1263, test_acc: 54.55, test_fscore: 0.0, time: 2.61 sec\n",
      "fold: 1, epoch: 2, train_loss: 2.8693, train_acc: 56.1, train_fscore: 0.0, test_loss: 2.4699, test_acc: 54.55, test_fscore: 0.0, time: 2.12 sec\n",
      "fold: 1, epoch: 3, train_loss: 2.373, train_acc: 39.02, train_fscore: 41.86, test_loss: 1.9379, test_acc: 72.73, test_fscore: 76.92, time: 2.24 sec\n",
      "fold: 1, epoch: 4, train_loss: 1.8711, train_acc: 63.41, train_fscore: 51.61, test_loss: 1.7456, test_acc: 54.55, test_fscore: 0.0, time: 2.17 sec\n",
      "fold: 1, epoch: 5, train_loss: 1.7311, train_acc: 58.54, train_fscore: 10.53, test_loss: 1.5757, test_acc: 81.82, test_fscore: 75.0, time: 2.26 sec\n",
      "fold: 1, epoch: 6, train_loss: 1.5842, train_acc: 63.41, train_fscore: 28.57, test_loss: 1.4859, test_acc: 54.55, test_fscore: 0.0, time: 2.19 sec\n",
      "fold: 1, epoch: 7, train_loss: 1.5141, train_acc: 70.73, train_fscore: 57.14, test_loss: 1.3959, test_acc: 90.91, test_fscore: 90.91, time: 2.24 sec\n",
      "fold: 1, epoch: 8, train_loss: 1.3966, train_acc: 87.8, train_fscore: 84.85, test_loss: 1.2907, test_acc: 81.82, test_fscore: 75.0, time: 2.17 sec\n",
      "fold: 1, epoch: 9, train_loss: 1.3209, train_acc: 78.05, train_fscore: 74.29, test_loss: 1.1805, test_acc: 81.82, test_fscore: 75.0, time: 2.24 sec\n",
      "fold: 1, epoch: 10, train_loss: 1.1943, train_acc: 87.8, train_fscore: 83.87, test_loss: 1.1144, test_acc: 90.91, test_fscore: 90.91, time: 2.35 sec\n",
      "fold: 1, epoch: 11, train_loss: 1.0832, train_acc: 92.68, train_fscore: 92.31, test_loss: 0.9591, test_acc: 90.91, test_fscore: 90.91, time: 2.29 sec\n",
      "fold: 1, epoch: 12, train_loss: 0.965, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0093, test_acc: 90.91, test_fscore: 88.89, time: 2.42 sec\n",
      "fold: 1, epoch: 13, train_loss: 0.899, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0361, test_acc: 90.91, test_fscore: 90.91, time: 2.43 sec\n",
      "fold: 1, epoch: 14, train_loss: 0.8634, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2045, test_acc: 81.82, test_fscore: 83.33, time: 2.38 sec\n",
      "fold: 1, epoch: 15, train_loss: 0.8437, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0978, test_acc: 90.91, test_fscore: 90.91, time: 2.35 sec\n",
      "fold: 1, epoch: 16, train_loss: 0.8211, train_acc: 100.0, train_fscore: 100.0, test_loss: 0.9855, test_acc: 90.91, test_fscore: 90.91, time: 2.31 sec\n",
      "fold: 1, epoch: 17, train_loss: 0.8121, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0159, test_acc: 90.91, test_fscore: 90.91, time: 2.37 sec\n",
      "fold: 1, epoch: 18, train_loss: 0.8039, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1716, test_acc: 81.82, test_fscore: 83.33, time: 2.35 sec\n",
      "fold: 1, epoch: 19, train_loss: 0.7937, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3315, test_acc: 81.82, test_fscore: 83.33, time: 2.36 sec\n",
      "fold: 1, epoch: 20, train_loss: 0.7877, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3485, test_acc: 81.82, test_fscore: 83.33, time: 2.36 sec\n",
      "fold: 1, epoch: 21, train_loss: 0.783, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.232, test_acc: 81.82, test_fscore: 83.33, time: 2.33 sec\n",
      "fold: 1, epoch: 22, train_loss: 0.7789, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1375, test_acc: 81.82, test_fscore: 83.33, time: 2.36 sec\n",
      "fold: 1, epoch: 23, train_loss: 0.7748, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1045, test_acc: 81.82, test_fscore: 83.33, time: 2.33 sec\n",
      "fold: 1, epoch: 24, train_loss: 0.7715, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1625, test_acc: 81.82, test_fscore: 83.33, time: 2.24 sec\n",
      "fold: 1, epoch: 25, train_loss: 0.769, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2383, test_acc: 81.82, test_fscore: 83.33, time: 2.12 sec\n",
      "fold: 1, epoch: 26, train_loss: 0.7671, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2565, test_acc: 81.82, test_fscore: 83.33, time: 2.3 sec\n",
      "fold: 1, epoch: 27, train_loss: 0.7658, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2008, test_acc: 81.82, test_fscore: 83.33, time: 2.2 sec\n",
      "fold: 1, epoch: 28, train_loss: 0.7632, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1491, test_acc: 81.82, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 1, epoch: 29, train_loss: 0.762, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1326, test_acc: 81.82, test_fscore: 83.33, time: 2.18 sec\n",
      "fold: 1, epoch: 30, train_loss: 0.7606, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1196, test_acc: 81.82, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 1, epoch: 31, train_loss: 0.7594, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1647, test_acc: 81.82, test_fscore: 83.33, time: 2.15 sec\n",
      "fold: 1, epoch: 32, train_loss: 0.7584, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1879, test_acc: 81.82, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 1, epoch: 33, train_loss: 0.7573, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1853, test_acc: 81.82, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 1, epoch: 34, train_loss: 0.7563, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1539, test_acc: 81.82, test_fscore: 83.33, time: 2.17 sec\n",
      "fold: 1, epoch: 35, train_loss: 0.7558, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1316, test_acc: 81.82, test_fscore: 83.33, time: 2.32 sec\n",
      "fold: 1, epoch: 36, train_loss: 0.7557, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1477, test_acc: 81.82, test_fscore: 83.33, time: 2.26 sec\n",
      "fold: 1, epoch: 37, train_loss: 0.7547, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1456, test_acc: 81.82, test_fscore: 83.33, time: 2.26 sec\n",
      "fold: 1, epoch: 38, train_loss: 0.7538, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1357, test_acc: 81.82, test_fscore: 83.33, time: 2.18 sec\n",
      "fold: 1, epoch: 39, train_loss: 0.7538, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1542, test_acc: 81.82, test_fscore: 83.33, time: 2.18 sec\n",
      "fold: 1, epoch: 40, train_loss: 0.7528, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1685, test_acc: 81.82, test_fscore: 83.33, time: 2.2 sec\n",
      "fold: 1, epoch: 41, train_loss: 0.7524, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1934, test_acc: 81.82, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 1, epoch: 42, train_loss: 0.752, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1967, test_acc: 81.82, test_fscore: 83.33, time: 2.17 sec\n",
      "fold: 1, epoch: 43, train_loss: 0.7517, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1834, test_acc: 81.82, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 1, epoch: 44, train_loss: 0.7512, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1679, test_acc: 81.82, test_fscore: 83.33, time: 2.18 sec\n",
      "fold: 1, epoch: 45, train_loss: 0.7507, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1626, test_acc: 81.82, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 1, epoch: 46, train_loss: 0.7501, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1833, test_acc: 81.82, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 1, epoch: 47, train_loss: 0.7497, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1981, test_acc: 81.82, test_fscore: 83.33, time: 2.28 sec\n",
      "fold: 1, epoch: 48, train_loss: 0.7496, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1805, test_acc: 81.82, test_fscore: 83.33, time: 2.14 sec\n",
      "fold: 1, epoch: 49, train_loss: 0.7494, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.168, test_acc: 81.82, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 1, epoch: 50, train_loss: 0.7492, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1389, test_acc: 81.82, test_fscore: 83.33, time: 2.18 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8333    0.9091         6\n",
      "           1     0.8333    1.0000    0.9091         5\n",
      "\n",
      "    accuracy                         0.9091        11\n",
      "   macro avg     0.9167    0.9167    0.9091        11\n",
      "weighted avg     0.9242    0.9091    0.9091        11\n",
      "\n",
      "[[5 1]\n",
      " [0 5]]\n",
      "best model saved in:  /home/zhhaibo2023/program/msce/code/run_model/modma_runs/01-20_23-49_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold1/saved_model/modma_model_fold1_acc.pt\n",
      "Test performance..\n",
      "fold: 1, F-Score: 90.91\n",
      "fold: 1, F-Score-index: 7\n",
      "fold: 1, Acc: 90.91\n",
      "fold: 1, Acc-index: 7\n",
      "bert_model_path: /home/zhhaibo2023/Pretrained_Model/bert-base-chinese\n",
      "Model: DepressionDetectionModel\n",
      "num_stimuls:  18\n",
      "fold: 2, epoch: 1, train_loss: 3.2038, train_acc: 51.22, train_fscore: 44.44, test_loss: 2.6287, test_acc: 27.27, test_fscore: 42.86, time: 2.66 sec\n",
      "fold: 2, epoch: 2, train_loss: 2.2385, train_acc: 56.1, train_fscore: 67.86, test_loss: 1.8718, test_acc: 81.82, test_fscore: 50.0, time: 2.27 sec\n",
      "fold: 2, epoch: 3, train_loss: 1.7807, train_acc: 63.41, train_fscore: 40.0, test_loss: 1.7372, test_acc: 54.55, test_fscore: 54.55, time: 2.25 sec\n",
      "fold: 2, epoch: 4, train_loss: 1.6455, train_acc: 60.98, train_fscore: 70.37, test_loss: 1.5525, test_acc: 81.82, test_fscore: 50.0, time: 2.12 sec\n",
      "fold: 2, epoch: 5, train_loss: 1.6366, train_acc: 53.66, train_fscore: 9.52, test_loss: 1.4128, test_acc: 81.82, test_fscore: 50.0, time: 2.15 sec\n",
      "fold: 2, epoch: 6, train_loss: 1.3896, train_acc: 90.24, train_fscore: 90.48, test_loss: 1.4378, test_acc: 90.91, test_fscore: 85.71, time: 2.46 sec\n",
      "fold: 2, epoch: 7, train_loss: 1.2544, train_acc: 92.68, train_fscore: 93.02, test_loss: 1.3405, test_acc: 81.82, test_fscore: 50.0, time: 2.13 sec\n",
      "fold: 2, epoch: 8, train_loss: 1.1471, train_acc: 95.12, train_fscore: 94.74, test_loss: 1.1978, test_acc: 81.82, test_fscore: 50.0, time: 2.22 sec\n",
      "fold: 2, epoch: 9, train_loss: 0.9977, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1659, test_acc: 81.82, test_fscore: 50.0, time: 2.15 sec\n",
      "fold: 2, epoch: 10, train_loss: 0.9225, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3622, test_acc: 81.82, test_fscore: 50.0, time: 2.11 sec\n",
      "fold: 2, epoch: 11, train_loss: 0.881, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1915, test_acc: 81.82, test_fscore: 50.0, time: 2.19 sec\n",
      "fold: 2, epoch: 12, train_loss: 0.8487, train_acc: 100.0, train_fscore: 100.0, test_loss: 0.9464, test_acc: 90.91, test_fscore: 80.0, time: 2.16 sec\n",
      "fold: 2, epoch: 13, train_loss: 0.8431, train_acc: 100.0, train_fscore: 100.0, test_loss: 0.9302, test_acc: 90.91, test_fscore: 80.0, time: 2.18 sec\n",
      "fold: 2, epoch: 14, train_loss: 0.8171, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3173, test_acc: 81.82, test_fscore: 50.0, time: 2.17 sec\n",
      "fold: 2, epoch: 15, train_loss: 0.8113, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3641, test_acc: 81.82, test_fscore: 50.0, time: 2.24 sec\n",
      "fold: 2, epoch: 16, train_loss: 0.7988, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1113, test_acc: 81.82, test_fscore: 50.0, time: 2.2 sec\n",
      "fold: 2, epoch: 17, train_loss: 0.7878, train_acc: 100.0, train_fscore: 100.0, test_loss: 0.9085, test_acc: 90.91, test_fscore: 80.0, time: 2.25 sec\n",
      "fold: 2, epoch: 18, train_loss: 0.7838, train_acc: 100.0, train_fscore: 100.0, test_loss: 0.9084, test_acc: 100.0, test_fscore: 100.0, time: 2.29 sec\n",
      "fold: 2, epoch: 19, train_loss: 0.779, train_acc: 100.0, train_fscore: 100.0, test_loss: 0.9679, test_acc: 81.82, test_fscore: 50.0, time: 2.2 sec\n",
      "fold: 2, epoch: 20, train_loss: 0.7733, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1689, test_acc: 81.82, test_fscore: 50.0, time: 2.21 sec\n",
      "fold: 2, epoch: 21, train_loss: 0.7707, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2742, test_acc: 81.82, test_fscore: 50.0, time: 2.13 sec\n",
      "fold: 2, epoch: 22, train_loss: 0.7682, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2285, test_acc: 81.82, test_fscore: 50.0, time: 2.18 sec\n",
      "fold: 2, epoch: 23, train_loss: 0.7654, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1089, test_acc: 81.82, test_fscore: 50.0, time: 2.17 sec\n",
      "fold: 2, epoch: 24, train_loss: 0.7637, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.036, test_acc: 81.82, test_fscore: 50.0, time: 2.23 sec\n",
      "fold: 2, epoch: 25, train_loss: 0.7621, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0623, test_acc: 81.82, test_fscore: 50.0, time: 2.18 sec\n",
      "fold: 2, epoch: 26, train_loss: 0.7603, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1794, test_acc: 81.82, test_fscore: 50.0, time: 2.24 sec\n",
      "fold: 2, epoch: 27, train_loss: 0.7582, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2388, test_acc: 81.82, test_fscore: 50.0, time: 2.22 sec\n",
      "fold: 2, epoch: 28, train_loss: 0.7578, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1868, test_acc: 81.82, test_fscore: 50.0, time: 2.25 sec\n",
      "fold: 2, epoch: 29, train_loss: 0.7565, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1562, test_acc: 81.82, test_fscore: 50.0, time: 2.18 sec\n",
      "fold: 2, epoch: 30, train_loss: 0.7553, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1461, test_acc: 81.82, test_fscore: 50.0, time: 2.17 sec\n",
      "fold: 2, epoch: 31, train_loss: 0.7544, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1471, test_acc: 81.82, test_fscore: 50.0, time: 2.16 sec\n",
      "fold: 2, epoch: 32, train_loss: 0.754, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1673, test_acc: 81.82, test_fscore: 50.0, time: 2.19 sec\n",
      "fold: 2, epoch: 33, train_loss: 0.7534, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2101, test_acc: 81.82, test_fscore: 50.0, time: 2.21 sec\n",
      "fold: 2, epoch: 34, train_loss: 0.7526, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2391, test_acc: 81.82, test_fscore: 50.0, time: 2.18 sec\n",
      "fold: 2, epoch: 35, train_loss: 0.7522, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1994, test_acc: 81.82, test_fscore: 50.0, time: 2.12 sec\n",
      "fold: 2, epoch: 36, train_loss: 0.7518, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1629, test_acc: 81.82, test_fscore: 50.0, time: 2.17 sec\n",
      "fold: 2, epoch: 37, train_loss: 0.7509, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.185, test_acc: 81.82, test_fscore: 50.0, time: 2.26 sec\n",
      "fold: 2, epoch: 38, train_loss: 0.7508, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2276, test_acc: 81.82, test_fscore: 50.0, time: 2.23 sec\n",
      "fold: 2, epoch: 39, train_loss: 0.7503, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2483, test_acc: 81.82, test_fscore: 50.0, time: 2.29 sec\n",
      "fold: 2, epoch: 40, train_loss: 0.7498, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2479, test_acc: 81.82, test_fscore: 50.0, time: 2.15 sec\n",
      "fold: 2, epoch: 41, train_loss: 0.7495, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2047, test_acc: 81.82, test_fscore: 50.0, time: 2.26 sec\n",
      "fold: 2, epoch: 42, train_loss: 0.749, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.162, test_acc: 81.82, test_fscore: 50.0, time: 2.2 sec\n",
      "fold: 2, epoch: 43, train_loss: 0.7487, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1677, test_acc: 81.82, test_fscore: 50.0, time: 2.22 sec\n",
      "fold: 2, epoch: 44, train_loss: 0.7486, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1992, test_acc: 81.82, test_fscore: 50.0, time: 2.26 sec\n",
      "fold: 2, epoch: 45, train_loss: 0.7479, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2426, test_acc: 81.82, test_fscore: 50.0, time: 2.25 sec\n",
      "fold: 2, epoch: 46, train_loss: 0.7478, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2589, test_acc: 81.82, test_fscore: 50.0, time: 2.22 sec\n",
      "fold: 2, epoch: 47, train_loss: 0.7475, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2057, test_acc: 81.82, test_fscore: 50.0, time: 2.35 sec\n",
      "fold: 2, epoch: 48, train_loss: 0.7471, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1783, test_acc: 81.82, test_fscore: 50.0, time: 2.18 sec\n",
      "fold: 2, epoch: 49, train_loss: 0.7467, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1996, test_acc: 81.82, test_fscore: 50.0, time: 2.23 sec\n",
      "fold: 2, epoch: 50, train_loss: 0.7467, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2345, test_acc: 81.82, test_fscore: 50.0, time: 2.22 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         8\n",
      "           1     1.0000    1.0000    1.0000         3\n",
      "\n",
      "    accuracy                         1.0000        11\n",
      "   macro avg     1.0000    1.0000    1.0000        11\n",
      "weighted avg     1.0000    1.0000    1.0000        11\n",
      "\n",
      "[[8 0]\n",
      " [0 3]]\n",
      "best model saved in:  /home/zhhaibo2023/program/msce/code/run_model/modma_runs/01-20_23-51_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold2/saved_model/modma_model_fold2_acc.pt\n",
      "Test performance..\n",
      "fold: 2, F-Score: 100.0\n",
      "fold: 2, F-Score-index: 18\n",
      "fold: 2, Acc: 100.0\n",
      "fold: 2, Acc-index: 18\n",
      "bert_model_path: /home/zhhaibo2023/Pretrained_Model/bert-base-chinese\n",
      "Model: DepressionDetectionModel\n",
      "num_stimuls:  18\n",
      "fold: 3, epoch: 1, train_loss: 3.4531, train_acc: 42.86, train_fscore: 29.41, test_loss: 2.8698, test_acc: 50.0, test_fscore: 0.0, time: 2.66 sec\n",
      "fold: 3, epoch: 2, train_loss: 2.94, train_acc: 57.14, train_fscore: 0.0, test_loss: 2.527, test_acc: 50.0, test_fscore: 0.0, time: 2.21 sec\n",
      "fold: 3, epoch: 3, train_loss: 2.4502, train_acc: 50.0, train_fscore: 27.59, test_loss: 1.8935, test_acc: 60.0, test_fscore: 33.33, time: 2.26 sec\n",
      "fold: 3, epoch: 4, train_loss: 1.9058, train_acc: 61.9, train_fscore: 27.27, test_loss: 1.6915, test_acc: 50.0, test_fscore: 0.0, time: 2.13 sec\n",
      "fold: 3, epoch: 5, train_loss: 1.7055, train_acc: 57.14, train_fscore: 0.0, test_loss: 1.5022, test_acc: 90.0, test_fscore: 88.89, time: 2.18 sec\n",
      "fold: 3, epoch: 6, train_loss: 1.5725, train_acc: 73.81, train_fscore: 66.67, test_loss: 1.4002, test_acc: 90.0, test_fscore: 88.89, time: 2.2 sec\n",
      "fold: 3, epoch: 7, train_loss: 1.4779, train_acc: 88.1, train_fscore: 85.71, test_loss: 1.465, test_acc: 60.0, test_fscore: 33.33, time: 2.14 sec\n",
      "fold: 3, epoch: 8, train_loss: 1.3642, train_acc: 76.19, train_fscore: 61.54, test_loss: 1.1011, test_acc: 100.0, test_fscore: 100.0, time: 2.32 sec\n",
      "fold: 3, epoch: 9, train_loss: 1.3236, train_acc: 78.57, train_fscore: 78.05, test_loss: 1.5999, test_acc: 60.0, test_fscore: 33.33, time: 2.22 sec\n",
      "fold: 3, epoch: 10, train_loss: 1.2255, train_acc: 83.33, train_fscore: 80.0, test_loss: 1.0008, test_acc: 90.0, test_fscore: 88.89, time: 2.29 sec\n",
      "fold: 3, epoch: 11, train_loss: 1.016, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.8348, test_acc: 70.0, test_fscore: 57.14, time: 2.4 sec\n",
      "fold: 3, epoch: 12, train_loss: 0.9588, train_acc: 97.62, train_fscore: 97.14, test_loss: 1.0575, test_acc: 90.0, test_fscore: 88.89, time: 2.38 sec\n",
      "fold: 3, epoch: 13, train_loss: 0.8913, train_acc: 100.0, train_fscore: 100.0, test_loss: 0.9239, test_acc: 90.0, test_fscore: 88.89, time: 2.33 sec\n",
      "fold: 3, epoch: 14, train_loss: 0.863, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5669, test_acc: 70.0, test_fscore: 57.14, time: 2.41 sec\n",
      "fold: 3, epoch: 15, train_loss: 0.8416, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.9653, test_acc: 70.0, test_fscore: 57.14, time: 2.37 sec\n",
      "fold: 3, epoch: 16, train_loss: 0.8293, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6506, test_acc: 70.0, test_fscore: 57.14, time: 2.26 sec\n",
      "fold: 3, epoch: 17, train_loss: 0.8112, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1885, test_acc: 90.0, test_fscore: 88.89, time: 2.4 sec\n",
      "fold: 3, epoch: 18, train_loss: 0.8025, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0313, test_acc: 90.0, test_fscore: 88.89, time: 2.17 sec\n",
      "fold: 3, epoch: 19, train_loss: 0.7958, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1237, test_acc: 90.0, test_fscore: 88.89, time: 2.18 sec\n",
      "fold: 3, epoch: 20, train_loss: 0.7889, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3983, test_acc: 80.0, test_fscore: 75.0, time: 2.18 sec\n",
      "fold: 3, epoch: 21, train_loss: 0.784, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6584, test_acc: 70.0, test_fscore: 57.14, time: 2.15 sec\n",
      "fold: 3, epoch: 22, train_loss: 0.7809, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7215, test_acc: 70.0, test_fscore: 57.14, time: 2.27 sec\n",
      "fold: 3, epoch: 23, train_loss: 0.7776, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5621, test_acc: 70.0, test_fscore: 57.14, time: 2.15 sec\n",
      "fold: 3, epoch: 24, train_loss: 0.7745, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3875, test_acc: 80.0, test_fscore: 75.0, time: 2.25 sec\n",
      "fold: 3, epoch: 25, train_loss: 0.7723, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3259, test_acc: 80.0, test_fscore: 75.0, time: 2.29 sec\n",
      "fold: 3, epoch: 26, train_loss: 0.7709, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3708, test_acc: 80.0, test_fscore: 75.0, time: 2.28 sec\n",
      "fold: 3, epoch: 27, train_loss: 0.7693, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5162, test_acc: 70.0, test_fscore: 57.14, time: 2.16 sec\n",
      "fold: 3, epoch: 28, train_loss: 0.7676, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6553, test_acc: 70.0, test_fscore: 57.14, time: 2.19 sec\n",
      "fold: 3, epoch: 29, train_loss: 0.7662, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7029, test_acc: 70.0, test_fscore: 57.14, time: 2.2 sec\n",
      "fold: 3, epoch: 30, train_loss: 0.7651, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.638, test_acc: 70.0, test_fscore: 57.14, time: 2.15 sec\n",
      "fold: 3, epoch: 31, train_loss: 0.7641, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5748, test_acc: 70.0, test_fscore: 57.14, time: 2.15 sec\n",
      "fold: 3, epoch: 32, train_loss: 0.7633, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5446, test_acc: 70.0, test_fscore: 57.14, time: 2.26 sec\n",
      "fold: 3, epoch: 33, train_loss: 0.7625, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5841, test_acc: 70.0, test_fscore: 57.14, time: 2.15 sec\n",
      "fold: 3, epoch: 34, train_loss: 0.7613, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6342, test_acc: 70.0, test_fscore: 57.14, time: 2.3 sec\n",
      "fold: 3, epoch: 35, train_loss: 0.7607, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6543, test_acc: 70.0, test_fscore: 57.14, time: 2.27 sec\n",
      "fold: 3, epoch: 36, train_loss: 0.76, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6364, test_acc: 70.0, test_fscore: 57.14, time: 2.21 sec\n",
      "fold: 3, epoch: 37, train_loss: 0.7599, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6136, test_acc: 70.0, test_fscore: 57.14, time: 2.26 sec\n",
      "fold: 3, epoch: 38, train_loss: 0.7591, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6442, test_acc: 70.0, test_fscore: 57.14, time: 2.27 sec\n",
      "fold: 3, epoch: 39, train_loss: 0.7587, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6705, test_acc: 70.0, test_fscore: 57.14, time: 2.16 sec\n",
      "fold: 3, epoch: 40, train_loss: 0.7582, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6833, test_acc: 70.0, test_fscore: 57.14, time: 2.2 sec\n",
      "fold: 3, epoch: 41, train_loss: 0.758, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6797, test_acc: 70.0, test_fscore: 57.14, time: 2.13 sec\n",
      "fold: 3, epoch: 42, train_loss: 0.7571, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6993, test_acc: 70.0, test_fscore: 57.14, time: 2.29 sec\n",
      "fold: 3, epoch: 43, train_loss: 0.7568, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7109, test_acc: 70.0, test_fscore: 57.14, time: 2.16 sec\n",
      "fold: 3, epoch: 44, train_loss: 0.7566, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7153, test_acc: 70.0, test_fscore: 57.14, time: 2.14 sec\n",
      "fold: 3, epoch: 45, train_loss: 0.7563, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7155, test_acc: 70.0, test_fscore: 57.14, time: 2.26 sec\n",
      "fold: 3, epoch: 46, train_loss: 0.756, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7362, test_acc: 70.0, test_fscore: 57.14, time: 2.13 sec\n",
      "fold: 3, epoch: 47, train_loss: 0.7556, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7364, test_acc: 70.0, test_fscore: 57.14, time: 2.23 sec\n",
      "fold: 3, epoch: 48, train_loss: 0.7554, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7697, test_acc: 70.0, test_fscore: 57.14, time: 2.19 sec\n",
      "fold: 3, epoch: 49, train_loss: 0.755, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7671, test_acc: 70.0, test_fscore: 57.14, time: 2.26 sec\n",
      "fold: 3, epoch: 50, train_loss: 0.7547, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.7671, test_acc: 70.0, test_fscore: 57.14, time: 2.22 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         5\n",
      "           1     1.0000    1.0000    1.0000         5\n",
      "\n",
      "    accuracy                         1.0000        10\n",
      "   macro avg     1.0000    1.0000    1.0000        10\n",
      "weighted avg     1.0000    1.0000    1.0000        10\n",
      "\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "best model saved in:  /home/zhhaibo2023/program/msce/code/run_model/modma_runs/01-20_23-53_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold3/saved_model/modma_model_fold3_acc.pt\n",
      "Test performance..\n",
      "fold: 3, F-Score: 100.0\n",
      "fold: 3, F-Score-index: 8\n",
      "fold: 3, Acc: 100.0\n",
      "fold: 3, Acc-index: 8\n",
      "bert_model_path: /home/zhhaibo2023/Pretrained_Model/bert-base-chinese\n",
      "Model: DepressionDetectionModel\n",
      "num_stimuls:  18\n",
      "fold: 4, epoch: 1, train_loss: 3.5835, train_acc: 33.33, train_fscore: 36.36, test_loss: 2.6711, test_acc: 50.0, test_fscore: 66.67, time: 2.6 sec\n",
      "fold: 4, epoch: 2, train_loss: 2.703, train_acc: 42.86, train_fscore: 29.41, test_loss: 2.4448, test_acc: 50.0, test_fscore: 0.0, time: 2.16 sec\n",
      "fold: 4, epoch: 3, train_loss: 2.1882, train_acc: 57.14, train_fscore: 0.0, test_loss: 1.8237, test_acc: 40.0, test_fscore: 40.0, time: 2.15 sec\n",
      "fold: 4, epoch: 4, train_loss: 1.8105, train_acc: 61.9, train_fscore: 33.33, test_loss: 1.6251, test_acc: 70.0, test_fscore: 72.73, time: 2.42 sec\n",
      "fold: 4, epoch: 5, train_loss: 1.6948, train_acc: 57.14, train_fscore: 59.09, test_loss: 1.5071, test_acc: 60.0, test_fscore: 60.0, time: 2.25 sec\n",
      "fold: 4, epoch: 6, train_loss: 1.5538, train_acc: 64.29, train_fscore: 28.57, test_loss: 1.4563, test_acc: 50.0, test_fscore: 0.0, time: 2.17 sec\n",
      "fold: 4, epoch: 7, train_loss: 1.4536, train_acc: 80.95, train_fscore: 71.43, test_loss: 1.3868, test_acc: 70.0, test_fscore: 72.73, time: 2.18 sec\n",
      "fold: 4, epoch: 8, train_loss: 1.3979, train_acc: 88.1, train_fscore: 85.71, test_loss: 1.2817, test_acc: 60.0, test_fscore: 60.0, time: 2.14 sec\n",
      "fold: 4, epoch: 9, train_loss: 1.2392, train_acc: 90.48, train_fscore: 87.5, test_loss: 1.3067, test_acc: 80.0, test_fscore: 83.33, time: 2.27 sec\n",
      "fold: 4, epoch: 10, train_loss: 1.1626, train_acc: 90.48, train_fscore: 90.0, test_loss: 1.1898, test_acc: 70.0, test_fscore: 72.73, time: 2.18 sec\n",
      "fold: 4, epoch: 11, train_loss: 1.0138, train_acc: 97.62, train_fscore: 97.14, test_loss: 1.2997, test_acc: 80.0, test_fscore: 83.33, time: 2.24 sec\n",
      "fold: 4, epoch: 12, train_loss: 0.9147, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2663, test_acc: 80.0, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 4, epoch: 13, train_loss: 0.8789, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2969, test_acc: 70.0, test_fscore: 66.67, time: 2.19 sec\n",
      "fold: 4, epoch: 14, train_loss: 0.8487, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4829, test_acc: 80.0, test_fscore: 83.33, time: 2.2 sec\n",
      "fold: 4, epoch: 15, train_loss: 0.8313, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3829, test_acc: 80.0, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 4, epoch: 16, train_loss: 0.8125, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1971, test_acc: 70.0, test_fscore: 66.67, time: 2.32 sec\n",
      "fold: 4, epoch: 17, train_loss: 0.8058, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.168, test_acc: 90.0, test_fscore: 90.91, time: 2.46 sec\n",
      "fold: 4, epoch: 18, train_loss: 0.7958, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4704, test_acc: 80.0, test_fscore: 83.33, time: 2.24 sec\n",
      "fold: 4, epoch: 19, train_loss: 0.7903, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3556, test_acc: 80.0, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 4, epoch: 20, train_loss: 0.7836, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1598, test_acc: 80.0, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 4, epoch: 21, train_loss: 0.781, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1225, test_acc: 90.0, test_fscore: 90.91, time: 2.3 sec\n",
      "fold: 4, epoch: 22, train_loss: 0.7771, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2317, test_acc: 80.0, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 4, epoch: 23, train_loss: 0.7743, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3081, test_acc: 80.0, test_fscore: 83.33, time: 2.36 sec\n",
      "fold: 4, epoch: 24, train_loss: 0.7714, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1932, test_acc: 80.0, test_fscore: 83.33, time: 2.23 sec\n",
      "fold: 4, epoch: 25, train_loss: 0.7695, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0985, test_acc: 80.0, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 4, epoch: 26, train_loss: 0.7681, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1013, test_acc: 80.0, test_fscore: 83.33, time: 2.26 sec\n",
      "fold: 4, epoch: 27, train_loss: 0.7664, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1712, test_acc: 80.0, test_fscore: 83.33, time: 2.18 sec\n",
      "fold: 4, epoch: 28, train_loss: 0.7651, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1954, test_acc: 80.0, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 4, epoch: 29, train_loss: 0.764, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1446, test_acc: 80.0, test_fscore: 83.33, time: 2.15 sec\n",
      "fold: 4, epoch: 30, train_loss: 0.7632, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1023, test_acc: 80.0, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 4, epoch: 31, train_loss: 0.7622, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1076, test_acc: 80.0, test_fscore: 83.33, time: 2.41 sec\n",
      "fold: 4, epoch: 32, train_loss: 0.7612, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1645, test_acc: 80.0, test_fscore: 83.33, time: 2.3 sec\n",
      "fold: 4, epoch: 33, train_loss: 0.7607, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1724, test_acc: 80.0, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 4, epoch: 34, train_loss: 0.76, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.167, test_acc: 80.0, test_fscore: 83.33, time: 2.2 sec\n",
      "fold: 4, epoch: 35, train_loss: 0.7597, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1115, test_acc: 80.0, test_fscore: 83.33, time: 2.27 sec\n",
      "fold: 4, epoch: 36, train_loss: 0.7591, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0772, test_acc: 80.0, test_fscore: 83.33, time: 2.24 sec\n",
      "fold: 4, epoch: 37, train_loss: 0.7587, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1267, test_acc: 80.0, test_fscore: 83.33, time: 2.18 sec\n",
      "fold: 4, epoch: 38, train_loss: 0.7581, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1844, test_acc: 80.0, test_fscore: 83.33, time: 2.17 sec\n",
      "fold: 4, epoch: 39, train_loss: 0.7576, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2004, test_acc: 80.0, test_fscore: 83.33, time: 2.2 sec\n",
      "fold: 4, epoch: 40, train_loss: 0.7572, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1117, test_acc: 80.0, test_fscore: 83.33, time: 2.2 sec\n",
      "fold: 4, epoch: 41, train_loss: 0.7568, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0927, test_acc: 80.0, test_fscore: 83.33, time: 2.34 sec\n",
      "fold: 4, epoch: 42, train_loss: 0.7564, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1631, test_acc: 80.0, test_fscore: 83.33, time: 2.4 sec\n",
      "fold: 4, epoch: 43, train_loss: 0.7561, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2041, test_acc: 80.0, test_fscore: 83.33, time: 2.3 sec\n",
      "fold: 4, epoch: 44, train_loss: 0.7557, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.178, test_acc: 80.0, test_fscore: 83.33, time: 2.21 sec\n",
      "fold: 4, epoch: 45, train_loss: 0.7553, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1304, test_acc: 80.0, test_fscore: 83.33, time: 2.13 sec\n",
      "fold: 4, epoch: 46, train_loss: 0.7552, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1325, test_acc: 80.0, test_fscore: 83.33, time: 2.32 sec\n",
      "fold: 4, epoch: 47, train_loss: 0.7549, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1888, test_acc: 80.0, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 4, epoch: 48, train_loss: 0.7548, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1518, test_acc: 80.0, test_fscore: 83.33, time: 2.11 sec\n",
      "fold: 4, epoch: 49, train_loss: 0.7542, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.0889, test_acc: 80.0, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 4, epoch: 50, train_loss: 0.7539, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1008, test_acc: 80.0, test_fscore: 83.33, time: 2.2 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8000    0.8889         5\n",
      "           1     0.8333    1.0000    0.9091         5\n",
      "\n",
      "    accuracy                         0.9000        10\n",
      "   macro avg     0.9167    0.9000    0.8990        10\n",
      "weighted avg     0.9167    0.9000    0.8990        10\n",
      "\n",
      "[[4 1]\n",
      " [0 5]]\n",
      "best model saved in:  /home/zhhaibo2023/program/msce/code/run_model/modma_runs/01-20_23-55_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold4/saved_model/modma_model_fold4_acc.pt\n",
      "Test performance..\n",
      "fold: 4, F-Score: 90.91\n",
      "fold: 4, F-Score-index: 17\n",
      "fold: 4, Acc: 90.0\n",
      "fold: 4, Acc-index: 17\n",
      "bert_model_path: /home/zhhaibo2023/Pretrained_Model/bert-base-chinese\n",
      "Model: DepressionDetectionModel\n",
      "num_stimuls:  18\n",
      "fold: 5, epoch: 1, train_loss: 3.4542, train_acc: 45.24, train_fscore: 14.81, test_loss: 2.4029, test_acc: 60.0, test_fscore: 33.33, time: 2.62 sec\n",
      "fold: 5, epoch: 2, train_loss: 2.3826, train_acc: 42.86, train_fscore: 47.83, test_loss: 1.9038, test_acc: 50.0, test_fscore: 66.67, time: 2.19 sec\n",
      "fold: 5, epoch: 3, train_loss: 1.9116, train_acc: 52.38, train_fscore: 54.55, test_loss: 1.7532, test_acc: 50.0, test_fscore: 0.0, time: 2.22 sec\n",
      "fold: 5, epoch: 4, train_loss: 1.7351, train_acc: 57.14, train_fscore: 0.0, test_loss: 1.698, test_acc: 50.0, test_fscore: 0.0, time: 2.2 sec\n",
      "fold: 5, epoch: 5, train_loss: 1.6831, train_acc: 57.14, train_fscore: 0.0, test_loss: 1.5518, test_acc: 50.0, test_fscore: 0.0, time: 2.28 sec\n",
      "fold: 5, epoch: 6, train_loss: 1.5838, train_acc: 71.43, train_fscore: 66.67, test_loss: 1.5206, test_acc: 50.0, test_fscore: 66.67, time: 2.15 sec\n",
      "fold: 5, epoch: 7, train_loss: 1.5234, train_acc: 64.29, train_fscore: 69.39, test_loss: 1.4835, test_acc: 50.0, test_fscore: 0.0, time: 2.13 sec\n",
      "fold: 5, epoch: 8, train_loss: 1.464, train_acc: 59.52, train_fscore: 10.53, test_loss: 1.4451, test_acc: 60.0, test_fscore: 33.33, time: 2.18 sec\n",
      "fold: 5, epoch: 9, train_loss: 1.3529, train_acc: 85.71, train_fscore: 80.0, test_loss: 1.4645, test_acc: 60.0, test_fscore: 71.43, time: 2.22 sec\n",
      "fold: 5, epoch: 10, train_loss: 1.2604, train_acc: 92.86, train_fscore: 91.89, test_loss: 1.3268, test_acc: 70.0, test_fscore: 66.67, time: 2.29 sec\n",
      "fold: 5, epoch: 11, train_loss: 1.1407, train_acc: 95.24, train_fscore: 94.44, test_loss: 1.4489, test_acc: 60.0, test_fscore: 71.43, time: 2.2 sec\n",
      "fold: 5, epoch: 12, train_loss: 0.9755, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1773, test_acc: 70.0, test_fscore: 72.73, time: 2.17 sec\n",
      "fold: 5, epoch: 13, train_loss: 0.9098, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4843, test_acc: 60.0, test_fscore: 71.43, time: 2.2 sec\n",
      "fold: 5, epoch: 14, train_loss: 0.8721, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4633, test_acc: 60.0, test_fscore: 71.43, time: 2.15 sec\n",
      "fold: 5, epoch: 15, train_loss: 0.8426, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.1411, test_acc: 90.0, test_fscore: 90.91, time: 2.32 sec\n",
      "fold: 5, epoch: 16, train_loss: 0.8283, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.2289, test_acc: 90.0, test_fscore: 90.91, time: 2.15 sec\n",
      "fold: 5, epoch: 17, train_loss: 0.8143, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6205, test_acc: 60.0, test_fscore: 71.43, time: 2.3 sec\n",
      "fold: 5, epoch: 18, train_loss: 0.8028, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4966, test_acc: 80.0, test_fscore: 83.33, time: 2.19 sec\n",
      "fold: 5, epoch: 19, train_loss: 0.7963, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5039, test_acc: 70.0, test_fscore: 76.92, time: 2.22 sec\n",
      "fold: 5, epoch: 20, train_loss: 0.7888, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5833, test_acc: 60.0, test_fscore: 71.43, time: 2.27 sec\n",
      "fold: 5, epoch: 21, train_loss: 0.7839, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.514, test_acc: 60.0, test_fscore: 71.43, time: 2.21 sec\n",
      "fold: 5, epoch: 22, train_loss: 0.7803, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3825, test_acc: 70.0, test_fscore: 72.73, time: 2.27 sec\n",
      "fold: 5, epoch: 23, train_loss: 0.7769, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4498, test_acc: 50.0, test_fscore: 61.54, time: 2.29 sec\n",
      "fold: 5, epoch: 24, train_loss: 0.7738, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5775, test_acc: 60.0, test_fscore: 71.43, time: 2.18 sec\n",
      "fold: 5, epoch: 25, train_loss: 0.7717, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.6347, test_acc: 60.0, test_fscore: 71.43, time: 2.18 sec\n",
      "fold: 5, epoch: 26, train_loss: 0.77, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5988, test_acc: 60.0, test_fscore: 71.43, time: 2.17 sec\n",
      "fold: 5, epoch: 27, train_loss: 0.7675, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5349, test_acc: 60.0, test_fscore: 71.43, time: 2.41 sec\n",
      "fold: 5, epoch: 28, train_loss: 0.7663, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.52, test_acc: 50.0, test_fscore: 61.54, time: 2.41 sec\n",
      "fold: 5, epoch: 29, train_loss: 0.765, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5741, test_acc: 60.0, test_fscore: 71.43, time: 2.27 sec\n",
      "fold: 5, epoch: 30, train_loss: 0.7641, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5951, test_acc: 60.0, test_fscore: 71.43, time: 2.2 sec\n",
      "fold: 5, epoch: 31, train_loss: 0.7629, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5228, test_acc: 60.0, test_fscore: 66.67, time: 2.15 sec\n",
      "fold: 5, epoch: 32, train_loss: 0.762, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5353, test_acc: 50.0, test_fscore: 61.54, time: 2.21 sec\n",
      "fold: 5, epoch: 33, train_loss: 0.7615, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5063, test_acc: 60.0, test_fscore: 66.67, time: 2.16 sec\n",
      "fold: 5, epoch: 34, train_loss: 0.7603, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5017, test_acc: 60.0, test_fscore: 66.67, time: 2.33 sec\n",
      "fold: 5, epoch: 35, train_loss: 0.7599, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5073, test_acc: 60.0, test_fscore: 66.67, time: 2.19 sec\n",
      "fold: 5, epoch: 36, train_loss: 0.7596, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5086, test_acc: 70.0, test_fscore: 76.92, time: 2.21 sec\n",
      "fold: 5, epoch: 37, train_loss: 0.7588, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5678, test_acc: 60.0, test_fscore: 71.43, time: 2.21 sec\n",
      "fold: 5, epoch: 38, train_loss: 0.7585, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5355, test_acc: 70.0, test_fscore: 76.92, time: 2.16 sec\n",
      "fold: 5, epoch: 39, train_loss: 0.7579, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5392, test_acc: 70.0, test_fscore: 76.92, time: 2.17 sec\n",
      "fold: 5, epoch: 40, train_loss: 0.7575, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5134, test_acc: 70.0, test_fscore: 76.92, time: 2.19 sec\n",
      "fold: 5, epoch: 41, train_loss: 0.7572, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5232, test_acc: 70.0, test_fscore: 76.92, time: 2.19 sec\n",
      "fold: 5, epoch: 42, train_loss: 0.7566, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4493, test_acc: 60.0, test_fscore: 66.67, time: 2.19 sec\n",
      "fold: 5, epoch: 43, train_loss: 0.7563, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4052, test_acc: 60.0, test_fscore: 66.67, time: 2.14 sec\n",
      "fold: 5, epoch: 44, train_loss: 0.756, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4016, test_acc: 60.0, test_fscore: 66.67, time: 2.18 sec\n",
      "fold: 5, epoch: 45, train_loss: 0.7555, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4707, test_acc: 70.0, test_fscore: 76.92, time: 2.21 sec\n",
      "fold: 5, epoch: 46, train_loss: 0.7551, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.5039, test_acc: 70.0, test_fscore: 76.92, time: 2.13 sec\n",
      "fold: 5, epoch: 47, train_loss: 0.7551, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4695, test_acc: 70.0, test_fscore: 76.92, time: 2.19 sec\n",
      "fold: 5, epoch: 48, train_loss: 0.7546, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.416, test_acc: 70.0, test_fscore: 76.92, time: 2.13 sec\n",
      "fold: 5, epoch: 49, train_loss: 0.7544, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.3989, test_acc: 60.0, test_fscore: 66.67, time: 2.22 sec\n",
      "fold: 5, epoch: 50, train_loss: 0.7543, train_acc: 100.0, train_fscore: 100.0, test_loss: 1.4095, test_acc: 70.0, test_fscore: 76.92, time: 2.33 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8000    0.8889         5\n",
      "           1     0.8333    1.0000    0.9091         5\n",
      "\n",
      "    accuracy                         0.9000        10\n",
      "   macro avg     0.9167    0.9000    0.8990        10\n",
      "weighted avg     0.9167    0.9000    0.8990        10\n",
      "\n",
      "[[4 1]\n",
      " [0 5]]\n",
      "best model saved in:  /home/zhhaibo2023/program/msce/code/run_model/modma_runs/01-20_23-57_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold5/saved_model/modma_model_fold5_acc.pt\n",
      "Test performance..\n",
      "fold: 5, F-Score: 90.91\n",
      "fold: 5, F-Score-index: 15\n",
      "fold: 5, Acc: 90.0\n",
      "fold: 5, Acc-index: 15\n",
      "5-fold ave fscore: 94.54599999999998\n",
      "5-fold ave acc: 94.18199999999999\n"
     ]
    }
   ],
   "source": [
    "test_label_list_perfold, test_pred_list_perfold, test_id_list_perfold, best_test_acc, best_test_fs = run(dataset=dataset,\n",
    "                                                                                                        dataset_name=dataset_name,\n",
    "                                                                                                        logdir_root=logdir_root,\n",
    "                                                                                                        classify_flag=classify_flag,\n",
    "                                                                                                        device=device,\n",
    "                                                                                                        gammas=gammas,\n",
    "                                                                                                        SEED=SEED,\n",
    "                                                                                                        BS=BS,\n",
    "                                                                                                        LR=LR,\n",
    "                                                                                                        Weight_Decay=Weight_Decay,\n",
    "                                                                                                        Dropout=Dropout,\n",
    "                                                                                                        EPOCHS=EPOCHS,\n",
    "                                                                                                        K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.1.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
